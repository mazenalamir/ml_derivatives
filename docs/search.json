[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML_derivatives",
    "section": "",
    "text": "ml_derivatives is a python module for the computation of high order derivatives (up to order 4 in the current implementation) despite relative noise to signal noise that might be as big as 7% or even 10%.\nTwo appealing features of the modules are:\nFor a complete description of the algorithm, see the reference paper cited below\nHere, Only the main elements are briefly explained for an easy and quick use of the module."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "ML_derivatives",
    "section": "Installation",
    "text": "Installation\nThe package will be made available soon through the standard installation script1:\npip install ml_derivatives"
  },
  {
    "objectID": "index.html#problem",
    "href": "index.html#problem",
    "title": "ML_derivatives",
    "section": "Problem statement",
    "text": "Problem statement\nGiven a sequence yn of N values representing successive noisy measurements of of a physical signal ywhich is acquired using fixed acquisition period dt over the time interval \\(I\\). We want to compute an estimation of the d-th derivative of the underlying signal over the same time interval through a python call of the following form:\n\\[\n\\dfrac{d^d}{dt^d}\\Bigl[y\\Bigr]\\Biggl\\vert_I = \\texttt{estimate\\_derivative(yn, dt, d, ...)}\n\\tag{1}\\]\nIn the current implementation, \\(d\\in \\{0,1,2,3,4\\}\\). The case with \\(d=0\\) corresponds to a denoising task (filtering)."
  },
  {
    "objectID": "index.html#generation",
    "href": "index.html#generation",
    "title": "ML_derivatives",
    "section": "Generating time-series with exact high derivatives",
    "text": "Generating time-series with exact high derivatives\nIn order to start getting hands-on the main call that addresses the above described problem, the ml_derivatives provides a handy function that enables to generate a time series with prescribed bandwidth, length and sampling period, so that one can start testing the performance of the derivatives estimator.\n\n\n\n\n\n\nNote\n\n\n\nIf you already have your time-series, this step would be unnecessary although it is always advisable to use this facility to get checkable results before being confident with the module. Indeed, having your own noisy signal, it is generally difficult to get the ground truth regarding the higher dervatives of the time-series.\n\n\nThis is done using the method generate_time_series of the Derivator class of the module as shown in the python script below.\nimport numpy as np\nfrom ml_derivatives.mld import Derivator\n\nnt = 5001       # The length of the time series to generate\nomega = 5.0     # Its bandwidth\ndt = 0.02       # The sampling acquisition period\n\n# Create an instance of the main Derivator class \n\nder = Derivator()\n\n# Generate a signal with a prescribed set of attributes \n\nt, Y = der.generate_a_time_series(nt, omega, dt)\n\n# Add the desired amount of noise to the signal.\n\nnoise_level = 0.05\nyn = noise_level * np.random.randn(len(Y[0])) + Y[0]\nNotice that the outputs of the generate_time_series method of the Derivatorclass are the time vector t and a list Y of five time-series representing the successive derivatives of the signal. More precisely:\n\nY[0] is the noise free signal to which the noise is to be added. Notice that this time series is always normalized so that its maximum value is lower than 1.0.\nY[i] for i=1,2,3,4 is the corresponding exact i-th derivative of the signal.\n\nNotice that these derivatives have to be estimated from the noisy version of the signal, namely yn that is computed in the last line of the above script.\n\n\n\n\n\n\nTip\n\n\n\nNotice how we separate the generation of the noise-free time-series from the process of adding noise. This enables to check several noise levels for the same time-series. This is because there is a randomness character inside the generate_time_series method of the Derivator class.\n\n\nThe plotting python code and the resulting time series are shown in the following tabs.\n\nPlotting code.The plots\n\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# prepare the plot \nfig = make_subplots(\n        rows=5, \n        cols=1,\n        subplot_titles=([f\"{i}-order derivatives\" for i in range(5)]), \n        )\n\n# iterate over the derivatives and plot the time-series\nfor i in range(5):\n    if i == 0:\n            fig.add_trace(go.Scatter(x=t, y=yn, mode='lines', \n                                    line=dict(color='Black'), \n                                    opacity=0.8), row=i+1, col=1)\n    fig.add_trace(go.Scatter(x=t, y=Y[i]), row=i+1, col=1)\n\n# unify the x-axis zoom and set the sizes\nfig.update_layout(\n    showlegend=False,\n    width=800,\n    height=1200,\n    xaxis2=dict(matches='x'),\n    xaxis3=dict(matches='x'),\n    xaxis4=dict(matches='x'),\n    xaxis5=dict(matches='x')\n)\n\n\nNotice that the first subplot shows both the noise-free and the noisy version of the time-series to be differentiated. (The plots are zoomable)."
  },
  {
    "objectID": "index.html#derivate",
    "href": "index.html#derivate",
    "title": "ML_derivatives",
    "section": "Computing the derivatives via the derivate method",
    "text": "Computing the derivatives via the derivate method\nThe main method that is exported by the ml_derivatives module is the derivate that is described in the present section.\n\nInput arguments\nThe table below describes the input arguments of the derivate method.\n\n\nInput arguments for the derivate method of the class ml_derivatives module.\n\n\n\n\n\n\n\nParameter\nDescription\nDefault\n\n\n\n\ny\nThe noisy time-series to be differentitated to the order d. This should be a numpy array.\n–\n\n\nd\nInteger inside [0,1,2,3,4] representing the derivation order to be computed. Notice that the method for computing the derivation order d is not based on the computation of the lower order derivatives.\n–\n\n\ndt\nThe sampling acquisition period that should be a float scalar number.\n–\n\n\nfit_before\nA boolean option instructing the algorithm to first evaluate the bandwidth of the time-series y or not before computing the derivatives. Upon the first call, the fit is necessarily done. For the next evaluation, the option is made available and set to False by default for computational efficiency reasons. The rationale is to avoid the bandwidth evaluation if the user thinks that the outcome of the first evaluation is still relevant. Swith it to Trueonly if changes in the bandwidth is expected compared to the previous fit.\nFalse\n\n\nassess\nA boolean option. When True a computation of statistics of error on the \\(0\\)-th derivatives is done and based on which if the force_level option is set to true, the closest level of noise among those for which models are available is computed and used in the derivative estimation. Otherwise the noise level 0.05is used by default.\nFalse\n\n\nforce_level\nA boolean instructing the algorithm to look for the appropriate noise-level’s model to be used in the estimation of the derivative when set to True. Otherwise the noise level 0.05is used by default. This option is relevant only if the previous assess option is set to True.\nFalse\n\n\nmute_warnings\nA boolean option which, when set to True, Suppresses the printing of some warning messages that are not necessarily useful in production although they can be informative in the preparation phase.\nTrue\n\n\n\n\n\n\nOutput arguments\nThe derivate method of the ml_derivatives module returns a tuple, say output with the following meanings\n\n\nOutput arguments for the derivate method of the class ml_derivatives module.\n\n\n\n\n\n\nparameters\nDescription\n\n\n\n\noutput[0]\nThe estimated derivatives of order d where d is the derivation order used in the call of the method.\n\n\noutput[1]\nThe sequence of confidence interval associated to the estimation.\n\n\noutput[2]\nA dictionary representing the statistics of errors on the \\(0\\)-derivative estimation. This is produced only if the assess input argument is set to Trueotherwise output[2]=None is returned."
  },
  {
    "objectID": "index.html#example",
    "href": "index.html#example",
    "title": "ML_derivatives",
    "section": "Examples of use",
    "text": "Examples of use\n\nUse the derivatives estimator\nThe following script uses the previously generated time-series, add several levels of noise \\(\\in \\{0.02, 0.04, 0.05, 0.07\\}\\) and for each resulting noisy time-series, computes the fives derivatives (\\(d=0,1,2,3,4\\)) and create the associated plotly figures.\nfor noise in [0.02, 0.04, 0.05, 0.07]:\n    yn = noise * np.random.randn(len(Y[0])) + Y[0]\n    fig = make_subplots(rows=5, cols=1, \n                        subplot_titles=(f'true and noisy signal / noise={noise}', \n                                        f'True vs estimated {1}-derivative', \n                                        f'True vs estimated {2}-derivative',\n                                        f'True vs estimated {3}-derivative',  \n                                        f'True vs estimated {4}-derivative'),\n                        shared_xaxes=True)\n\n    fig.add_trace(go.Scatter(x=t, y=Y[0], name=f'True y'), row=1, col=1)\n    fig.add_trace(go.Scatter(x=t, y=yn, name=f'noisy yn'), row=1, col=1)\n\n    for d in range(5):\n        yhat, e_std, log = der.derivate(y=yn, d=d, dt=dt)\n        #--\n        fig.add_trace(go.Scatter(x=t, y=Y[d], name=f'True {d}-derivative', \n                                    line=dict(color='Black')), row=d+1, col=1)\n        \n        fig.add_trace(go.Scatter(x=t, y=yhat, name=f'estimated {d}-derivative', \n                                    line=dict(color='Blue')), row=d+1, col=1)\n\nfig.update_layout(\n    width = 800, \n    height=1200,\n    showlegend=False\n)\nfig.write_html(f'example_{int(100 * noise)}.html')\nThe resulting plots are shown in the tabls below:\n\nlevel noise = 0.02level noise = 0.04level noise = 0.05level noise = 0.07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the confidence intervals provided\nThe confidence interval is to be used as if was an indication avout the standard deviation that is defined point-wise along the time interval over which the estimation of the derivative is conducted. Based on this, it is common to use as confidence intervals around the estimation that show a width of mutiples of the standard deviation. This takes the following form:\n\\[\n[y_\\text{prediced}-\\rho\\sigma, y_\\text{prediced}+\\rho\\sigma]\\quad \\rho=1,2,3\n\\tag{2}\\]\nThe following script shows how the estimation of \\(\\sigma\\) can be used and computes for a noisy signal the probability of the true value being inside the above defined interval for different values of \\(\\rho\\in \\{1,2,3\\}\\). The thrid derivatice is used for the sake of illustration together with a noise level of 0.05.\nnoise = 0.05\nd = 3\nyn = noise * np.random.randn(len(Y[0])) + Y[0]\nyhat, sigma, _ = der.derivate(y=yn, d=d, dt=dt, assess=True, force_level=True)\ndic_confidence = {}\nfor rho in [1,2,3]:\n     cond1 = Y[d] &lt;= yhat + rho * sigma\n     cond2 = Y[d] &gt;= yhat - rho * sigma\n     dic_confidence[rho] = int(np.sum(cond1 & cond2)/len(yhat) * 100)\n\nimport pandas as pd \ndf_confidence = pd.DataFrame(dic_confidence, index=['Probability'])\ndf_confidence.columns = [f'rho={i+1}' for i in range(3)]\nprint(\"--------------------------------------------------------------\")\nprint(\"-- Probability of being inside [-rho * sigma, +rho * sigma] --\")\nprint(\"--------------------------------------------------------------\")\nprint(df_confidence)\nprint(\"--------------------------------------------------------------\")\nwhich produces the following result:\n\n\n\nRelevance of the confidence intervals\n\n\nwhich means for instance that more than 98% of the instants, the true values of the third derivatives lies inside the \\(2\\sigma\\)-defined confidence interval."
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "ML_derivatives",
    "section": "Limitations",
    "text": "Limitations\n\n\n\n\n\n\nMinimum number of N=50 points required.\n\n\n\nAs the derivatives estimator involves an estimation of the bandwidth of the signal and since the precision of this estimation is inversly proportional to the number of points, the current implementation of the module required a minimum number od points N=50.\n\n\n\n\n\n\n\n\nMaximum bandwidth for a given dt.\n\n\n\nObviously, given the acquisition period dt, there is a limitation on the bandwidth of the signal for which derivatives can be computed. For this reason, it is not advisable to attempt estimation beyond the following maximum pulsation: \\[\n\\omega_\\text{max}= \\dfrac{2\\pi}{5\\times \\texttt{dt}}\n\\]\n\n\n\n\n\n\n\n\nml_derivatives is slower than standard filters although incomparatively more precise\n\n\n\nIt is important to underline that the counterpart of the above nice features is that the processing time is not that of a point-wise filter although computation time remains descently small. See below for more details regarding standard compuation times using the ml_derivatives module.\n\n\n\n\n\n\n\n\nThe length of the time-series is limited to 10000\n\n\n\nSince the estimation is based on some stored information, for the sake of memory, the length of the sequence yn used as input argument in (Equation 1) is limited to 10000. If estimation of derivatives for longer sequence is required, please decompose the signal into multiple segments."
  },
  {
    "objectID": "index.html#citing",
    "href": "index.html#citing",
    "title": "ML_derivatives",
    "section": "Citing ml_derivatives",
    "text": "Citing ml_derivatives\n@misc{alamir2025reconstructinghighderivativesnoisy,\n      title={On reconstructing high derivatives of noisy time-series with confidence intervals}, \n      author={Mazen Alamir},\n      year={2025},\n      eprint={2503.05222},\n      archivePrefix={arXiv},\n      primaryClass={eess.SY},\n      url={https://arxiv.org/abs/2503.05222}, \n}\n\n\n\n\n\n\nNote\n\n\n\nThe above reference contains a detailed description of the algorithm together with an extensive comparison with the best available alternatives. It also explain the general targeted scope of the module that mainly focus on extracting high derivatives from noisy time-series in the aim of building learning datasets that contains virtual sensor-like columns representing derivatives of different orders of the raw columned coming from the ground measurements."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "ML_derivatives",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nShould be available in May 2025.↩︎"
  }
]