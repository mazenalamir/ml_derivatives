{"title":"ML_derivatives","markdown":{"yaml":{"title":"ML_derivatives","subtitle":"A module to compute high derivatives of noisy time-series.","author":[{"name":"Mazen Alamir","affiliation":"CNRS, University of Grenoble Alpes","homepage":"https://www.mazenalamir.fr"}],"date":"April 25, 2025","keywords":["Signal preocessing","derivatives estimation","denoising","Machine Learning","Python"]},"headingText":"Noise 5% / 1.94Hz","containsRefs":false,"markdown":"\n\n---\n`ml_derivatives` is a python module for the computation of high order derivatives (up to order 4 in the current implementation) despite relative noise to signal noise that might be as big as 7% or even 10%. \n\nTwo appealing features of the modules are: \n\n1. **Automatic tuning**: \n   \n    The `ml_derviatives` module enables an automatic tuning of the trade-off involving the bandwidth of the time-series and the level of noise affecting the measurements. While handling this trade-of is the major issue in derivating noisy signal,  this feature seems to be absent from the currently available alternatives which always come with tuning parameters that are left to the user. \n   \n1. **Computation of confidence intervals**:  \n\n    the `ml_derviatives` module provides (optionally) an estimation of an interval of confidence to which belong the true values of the derivatives with high probabiliy. This might be of great value when the estimated derivatives are extracted to build a model involving the high derivatives. Indeed, regions where the confidence interval is large might be de-selected in order to be excluded from the so-called *training dataset*.\n\n::: {.callout-tip collapse=\"true\" title='Tipycal comparison results with available alternatives (unfold to see)'}\n\n![Typical comparison results with available alternatives](images/comparison.png){fig-align=\"center\" width=\"80%\"}\n\n:::\n\n::: {.callout-tip collapse=\"true\" title='Example of derivatives reconstruction with confidence intervals'}\n\n::: {.panel-tabset}\n\n\n\n![Example of derivatives reconstruction with confidence intervals](images/derivatives_reconstruction_1.png){fig-align=\"center\" width=\"80%\"}\n\n\n### Noise 5% / 12.54Hz\n\n![Example of derivatives reconstruction with confidence intervals](images/derivatives_reconstruction_2.png){fig-align=\"center\" width=\"80%\"}\n\n### Noise 10% / 4.22Hz\n\n![Example of derivatives reconstruction with confidence intervals](images/derivatives_reconstruction_3.png){fig-align=\"center\" width=\"80%\"}\n\n:::\n\n:::\n\nFor a complete description of the algorithm, see the reference paper [cited below](#citing)\n\nHere, Only the main elements are briefly explained for an easy and quick use of the module. \n\n## Installation \n\nThe package will be made available soon through the standard installation script[^1]:\n\n```default\npip install ml-derivatives\n```\n\n## Problem statement {#problem}\nGiven a sequence `yn` of `N` values representing successive noisy measurements of of a physical signal `y`which is acquired using  **fixed acquisition period** `dt` over the time interval $I$. We want to compute an estimation of the `d`-th derivative of the underlying signal over the same time interval through a python call of the following form:  \n\n$$\n\\dfrac{d^d}{dt^d}\\Bigl[y\\Bigr]\\Biggl\\vert_I = \\texttt{estimate\\_derivative(yn, dt, d, ...)}\n$${#eq-problem}\n\nIn the current implementation, $d\\in \\{0,1,2,3,4\\}$. The case with $d=0$ corresponds to a denoising task (filtering). \n\n## Generating time-series with exact high derivatives {#generation}\n\nIn order to start getting hands-on the main call that addresses the above described problem, the `ml_derivatives` provides a handy function that enables to generate a time series with prescribed bandwidth, length and sampling period, so that one can start testing the performance of the derivatives estimator. \n\n::: {.callout-note}\nIf you already have your time-series, this step would be unnecessary although it is always advisable to use this facility to get *checkable* results before being confident with the module. Indeed, having your own noisy signal, it is generally difficult to get the ground truth regarding the higher dervatives of the time-series. \n:::\n\nThis is done using the method `generate_time_series` of the `Derivator` class of the module as shown in the python script below. \n\n```python\nimport numpy as np\nfrom ml_derivatives.mld import Derivator\n\nnt = 5001       # The length of the time series to generate\nomega = 5.0     # Its bandwidth\ndt = 0.02       # The sampling acquisition period\n\n# Create an instance of the main Derivator class \n\nder = Derivator()\n\n# Generate a signal with a prescribed set of attributes \n\nt, Y = der.generate_a_time_series(nt, omega, dt)\n\n# Add the desired amount of noise to the signal.\n# Remember that the generated time series Y[0] is normalized\n\nnoise_level = 0.05\nyn = noise_level * np.random.randn(len(Y[0])) + Y[0]\n```\n\nNotice that the outputs of the `generate_time_series` method of the `Derivator`class are the time vector `t` and a list `Y` of five time-series representing the successive derivatives of the signal. More precisely:\n\n- `Y[0]` is the noise free signal to which the noise is to be added. Notice that this time series is always normalized so that its maximum value is lower than 1.0.\n  \n- `Y[i]` for `i=1,2,3,4` is the corresponding exact i-th derivative of the signal.\n\nNotice that these derivatives have to be estimated from the noisy version of the signal, namely `yn` that is computed in the last line of the above script. \n\n::: {.callout-tip}\nNotice how we separate the generation of the noise-free time-series  from the process of adding noise. This enables to check several noise levels for the same time-series. This is because there is a randomness character inside the `generate_time_series` method of the `Derivator` class.\n:::\n\nThe plotting python code and the resulting time series are shown in the following tabs.\n\n::: {.panel-tabset}\n\n### Plotting code.\n\n```python\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# prepare the plot \nfig = make_subplots(\n        rows=5, \n        cols=1,\n        subplot_titles=([f\"{i}-order derivatives\" for i in range(5)]), \n        )\n\n# iterate over the derivatives and plot the time-series\nfor i in range(5):\n    if i == 0:\n            fig.add_trace(go.Scatter(x=t, y=yn, mode='lines', \n                                    line=dict(color='Black'), \n                                    opacity=0.8), row=i+1, col=1)\n    fig.add_trace(go.Scatter(x=t, y=Y[i]), row=i+1, col=1)\n\n# unify the x-axis zoom and set the sizes\nfig.update_layout(\n    showlegend=False,\n    width=800,\n    height=1200,\n    xaxis2=dict(matches='x'),\n    xaxis3=dict(matches='x'),\n    xaxis4=dict(matches='x'),\n    xaxis5=dict(matches='x')\n)\n```\n\n### The plots \n\nNotice that the first subplot shows both the noise-free and the noisy version of the time-series to be differentiated. (*The plots are zoomable*). \n\n:::{.embed}\n<iframe src=\"images/generated_plots.html\" width=\"1400\" height=\"800\"></iframe>\n:::\n\n:::\n\n## Computing the derivatives via the `derivate` method {#derivate}\n\nThe main method that is exported by the `ml_derivatives` module is the `derivate` that is described in the present section. \n\n### Input arguments \nThe table below describes the input arguments of the `derivate` method.\n\n:::{.tbl-caption}\n| **Parameter**     | **Description**      | **Default**|\n|---|---------------|----:|\n| `y` |  The noisy time-series to be differentitated to the order `d`. This should be a `numpy` array.| --|\n| `d`| Integer inside [0,1,2,3,4] representing the derivation order to be computed. Notice that the method for computing the derivation order `d` is not based on the computation of the lower order derivatives. | -- |\n| `dt`| The sampling acquisition period that should be a `float` scalar number. | -- |\n| `fit_before`| A boolean option instructing the algorithm to first evaluate the bandwidth of the time-series `y` or not before computing the derivatives. **Upon the first call, the fit is necessarily done**. For the next evaluation, the option is made available and set to `False` by default for computational efficiency reasons. The rationale is to avoid the bandwidth evaluation if the user thinks that the outcome of the first evaluation is still relevant. Swith it to `True`only if changes in the bandwidth is expected compared to the previous fit. | `False` |\n| `assess`| A boolean option. When `True` a computation of statistics of error on the $0$-th derivatives is done and based on which if the `force_level` option is set to true, the closest level of noise among those for which models are available is computed and used in the derivative estimation. Otherwise the noise level `0.05`is used by default. | `False` |\n| `force_level`| A boolean instructing the algorithm to look for the appropriate noise-level's model to be used in the estimation of the derivative when set to `True`. Otherwise the noise level `0.05`is used by default. This option is relevant only if the previous `assess` option is set to `True`. | `False` |\n| `mute_warnings`| A boolean option which, when set to `True`, Suppresses the printing of some warning messages that are not necessarily useful in production although they can be informative in the preparation phase. | `True` |\n\nTable: Input arguments for the `derivate` method of the class `ml_derivatives` module.\n:::\n\n### Output arguments \n\nThe `derivate` method of the `ml_derivatives` module returns a tuple, say `output` with the following meanings\n\n:::{.tbl-caption}\n| **parameters** | **Description** |\n|---|----------------|\n| `output[0]`| The estimated derivatives of order `d` where `d` is the derivation order used in the call of the method.|\n| `output[1]` | The sequence of confidence interval associated to the estimation. |\n| `output[2]` | A dictionary representing the statistics of errors on the $0$-derivative estimation. This is produced only if the `assess` input argument is set to `True`otherwise `output[2]=None` is returned. |\n\n\nTable: Output arguments for the `derivate` method of the class `ml_derivatives` module.\n:::\n\n## Examples of use {#example}\n\n### Use the derivatives estimator \n\nThe following script uses the previously generated time-series, add several levels of noise $\\in \\{0.02, 0.04, 0.05, 0.07\\}$ and for each resulting noisy time-series, computes the fives derivatives ($d=0,1,2,3,4$) and create the associated `plotly` figures. \n\n```python \nfor noise in [0.02, 0.04, 0.05, 0.07]:\n    yn = noise * np.random.randn(len(Y[0])) + Y[0]\n    fig = make_subplots(rows=5, cols=1, \n                        subplot_titles=(f'true and noisy signal / noise={noise}', \n                                        f'True vs estimated {1}-derivative', \n                                        f'True vs estimated {2}-derivative',\n                                        f'True vs estimated {3}-derivative',  \n                                        f'True vs estimated {4}-derivative'),\n                        shared_xaxes=True)\n\n    fig.add_trace(go.Scatter(x=t, y=Y[0], name=f'True y'), row=1, col=1)\n    fig.add_trace(go.Scatter(x=t, y=yn, name=f'noisy yn'), row=1, col=1)\n\n    for d in range(5):\n        yhat, e_std, log = der.derivate(y=yn, d=d, dt=dt)\n        #--\n        fig.add_trace(go.Scatter(x=t, y=Y[d], name=f'True {d}-derivative', \n                                    line=dict(color='Black')), row=d+1, col=1)\n        \n        fig.add_trace(go.Scatter(x=t, y=yhat, name=f'estimated {d}-derivative', \n                                    line=dict(color='Blue')), row=d+1, col=1)\n\nfig.update_layout(\n    width = 800, \n    height=1200,\n    showlegend=False\n)\nfig.write_html(f'example_{int(100 * noise)}.html')\n```\n\nThe resulting plots are shown in the tabls below: \n\n::: {.panel-tabset}\n\n### level noise = 0.02\n\n:::{.embed}\n<iframe src=\"images/example_2.html\" width=\"1400\" height=\"800\"></iframe>\n:::\n\n### level noise = 0.04\n\n:::{.embed}\n<iframe src=\"images/example_4.html\" width=\"1400\" height=\"800\"></iframe>\n:::\n\n### level noise = 0.05\n\n:::{.embed}\n<iframe src=\"images/example_5.html\" width=\"1400\" height=\"800\"></iframe>\n:::\n\n### level noise = 0.07\n\n:::{.embed}\n<iframe src=\"images/example_7.html\" width=\"1400\" height=\"800\"></iframe>\n:::\n\n:::\n\n### Use the confidence intervals provided \n\nThe confidence interval is to be used as if was an indication avout the standard deviation that is defined *point-wise* along the time interval over which the estimation of the derivative is conducted. Based on this, it is common to use as confidence intervals around the estimation that show a width of **mutiples** of the standard deviation. This takes the following form:\n\n$$\n[y_\\text{prediced}-\\rho\\sigma, y_\\text{prediced}+\\rho\\sigma]\\quad \\rho=1,2,3\n$${#eq-sigma}\n\nThe following script shows how the estimation of $\\sigma$ can be used and computes for a noisy signal the **probability** of the true value being inside the above defined interval for different values of $\\rho\\in \\{1,2,3\\}$. The thrid derivatice is used for the sake of illustration together with a noise level of 0.05. \n\n```python \nnoise = 0.05\nd = 3\nyn = noise * np.random.randn(len(Y[0])) + Y[0]\nyhat, sigma, _ = der.derivate(y=yn, d=d, dt=dt, assess=True, force_level=True)\ndic_confidence = {}\nfor rho in [1,2,3]:\n     cond1 = Y[d] <= yhat + rho * sigma\n     cond2 = Y[d] >= yhat - rho * sigma\n     dic_confidence[rho] = int(np.sum(cond1 & cond2)/len(yhat) * 100)\n\nimport pandas as pd \ndf_confidence = pd.DataFrame(dic_confidence, index=['Probability'])\ndf_confidence.columns = [f'rho={i+1}' for i in range(3)]\nprint(\"--------------------------------------------------------------\")\nprint(\"-- Probability of being inside [-rho * sigma, +rho * sigma] --\")\nprint(\"--------------------------------------------------------------\")\nprint(df_confidence)\nprint(\"--------------------------------------------------------------\")\n```\n\nwhich produces the following result: \n\n![Relevance of the confidence intervals](images/outputterminal.png){fig-align=\"left\" width=\"80%\"}\n\nwhich means for instance that **more than 98%** of the instants, the true values of the third derivatives lies inside the $2\\sigma$-defined confidence interval. \n\n\n## Limitations {#limitations}\n\n::: {.callout-warning title=\"Minimum number of `N`=50 points required.\"} \n\nAs the derivatives estimator involves an estimation of the bandwidth of the signal and since the precision of this estimation is inversly proportional to the number of points, the current implementation of the module required a minimum number od points `N`=50.\n\n:::\n\n\n::: {.callout-warning title=\"Maximum bandwidth for a given `dt`.\"} \n\nObviously, given the acquisition period `dt`, there is a limitation on the bandwidth of the signal for which derivatives can be computed. For this reason, it is not advisable to attempt estimation beyond the following maximum pulsation: \n$$\n\\omega_\\text{max}= \\dfrac{2\\pi}{5\\times \\texttt{dt}}\n$$\n\n:::\n\n::: {.callout-warning title=\"`ml_derivatives` is slower than standard filters although incomparatively more precise\"} \n\nIt is important to underline that the counterpart of the above nice features is that the processing time is not that of a point-wise filter although computation time remains descently small. See below for more details regarding standard compuation times using the `ml_derivatives` module. \n:::\n\n::: {.callout-warning title='The length of the time-series is limited to 10000'}\n\nSince the estimation is based on some stored information, for the sake of memory, the length of the sequence `yn` used as input argument in (@eq-problem) is limited to 10000. If estimation of derivatives for longer sequence is required, please decompose the signal into multiple segments. \n:::\n\n## Citing ml_derivatives {#citing}\n\n\n```bibtex\n@misc{alamir2025reconstructinghighderivativesnoisy,\n      title={On reconstructing high derivatives of noisy time-series with confidence intervals}, \n      author={Mazen Alamir},\n      year={2025},\n      eprint={2503.05222},\n      archivePrefix={arXiv},\n      primaryClass={eess.SY},\n      url={https://arxiv.org/abs/2503.05222}, \n}\n```\n\n::: {.callout-note}\nThe above reference contains a detailed description of the algorithm together with an extensive comparison with the best available alternatives. It also explain the general targeted scope of the module that mainly focus on extracting high derivatives from noisy time-series in the aim of building learning datasets that contains *virtual sensor*-like columns representing derivatives of different orders of the raw columned coming from the ground measurements. \n:::\n\n[^1]: Should be available in May 2025.","srcMarkdownNoYaml":"\n\n---\n`ml_derivatives` is a python module for the computation of high order derivatives (up to order 4 in the current implementation) despite relative noise to signal noise that might be as big as 7% or even 10%. \n\nTwo appealing features of the modules are: \n\n1. **Automatic tuning**: \n   \n    The `ml_derviatives` module enables an automatic tuning of the trade-off involving the bandwidth of the time-series and the level of noise affecting the measurements. While handling this trade-of is the major issue in derivating noisy signal,  this feature seems to be absent from the currently available alternatives which always come with tuning parameters that are left to the user. \n   \n1. **Computation of confidence intervals**:  \n\n    the `ml_derviatives` module provides (optionally) an estimation of an interval of confidence to which belong the true values of the derivatives with high probabiliy. This might be of great value when the estimated derivatives are extracted to build a model involving the high derivatives. Indeed, regions where the confidence interval is large might be de-selected in order to be excluded from the so-called *training dataset*.\n\n::: {.callout-tip collapse=\"true\" title='Tipycal comparison results with available alternatives (unfold to see)'}\n\n![Typical comparison results with available alternatives](images/comparison.png){fig-align=\"center\" width=\"80%\"}\n\n:::\n\n::: {.callout-tip collapse=\"true\" title='Example of derivatives reconstruction with confidence intervals'}\n\n::: {.panel-tabset}\n\n### Noise 5% / 1.94Hz\n\n\n![Example of derivatives reconstruction with confidence intervals](images/derivatives_reconstruction_1.png){fig-align=\"center\" width=\"80%\"}\n\n\n### Noise 5% / 12.54Hz\n\n![Example of derivatives reconstruction with confidence intervals](images/derivatives_reconstruction_2.png){fig-align=\"center\" width=\"80%\"}\n\n### Noise 10% / 4.22Hz\n\n![Example of derivatives reconstruction with confidence intervals](images/derivatives_reconstruction_3.png){fig-align=\"center\" width=\"80%\"}\n\n:::\n\n:::\n\nFor a complete description of the algorithm, see the reference paper [cited below](#citing)\n\nHere, Only the main elements are briefly explained for an easy and quick use of the module. \n\n## Installation \n\nThe package will be made available soon through the standard installation script[^1]:\n\n```default\npip install ml-derivatives\n```\n\n## Problem statement {#problem}\nGiven a sequence `yn` of `N` values representing successive noisy measurements of of a physical signal `y`which is acquired using  **fixed acquisition period** `dt` over the time interval $I$. We want to compute an estimation of the `d`-th derivative of the underlying signal over the same time interval through a python call of the following form:  \n\n$$\n\\dfrac{d^d}{dt^d}\\Bigl[y\\Bigr]\\Biggl\\vert_I = \\texttt{estimate\\_derivative(yn, dt, d, ...)}\n$${#eq-problem}\n\nIn the current implementation, $d\\in \\{0,1,2,3,4\\}$. The case with $d=0$ corresponds to a denoising task (filtering). \n\n## Generating time-series with exact high derivatives {#generation}\n\nIn order to start getting hands-on the main call that addresses the above described problem, the `ml_derivatives` provides a handy function that enables to generate a time series with prescribed bandwidth, length and sampling period, so that one can start testing the performance of the derivatives estimator. \n\n::: {.callout-note}\nIf you already have your time-series, this step would be unnecessary although it is always advisable to use this facility to get *checkable* results before being confident with the module. Indeed, having your own noisy signal, it is generally difficult to get the ground truth regarding the higher dervatives of the time-series. \n:::\n\nThis is done using the method `generate_time_series` of the `Derivator` class of the module as shown in the python script below. \n\n```python\nimport numpy as np\nfrom ml_derivatives.mld import Derivator\n\nnt = 5001       # The length of the time series to generate\nomega = 5.0     # Its bandwidth\ndt = 0.02       # The sampling acquisition period\n\n# Create an instance of the main Derivator class \n\nder = Derivator()\n\n# Generate a signal with a prescribed set of attributes \n\nt, Y = der.generate_a_time_series(nt, omega, dt)\n\n# Add the desired amount of noise to the signal.\n# Remember that the generated time series Y[0] is normalized\n\nnoise_level = 0.05\nyn = noise_level * np.random.randn(len(Y[0])) + Y[0]\n```\n\nNotice that the outputs of the `generate_time_series` method of the `Derivator`class are the time vector `t` and a list `Y` of five time-series representing the successive derivatives of the signal. More precisely:\n\n- `Y[0]` is the noise free signal to which the noise is to be added. Notice that this time series is always normalized so that its maximum value is lower than 1.0.\n  \n- `Y[i]` for `i=1,2,3,4` is the corresponding exact i-th derivative of the signal.\n\nNotice that these derivatives have to be estimated from the noisy version of the signal, namely `yn` that is computed in the last line of the above script. \n\n::: {.callout-tip}\nNotice how we separate the generation of the noise-free time-series  from the process of adding noise. This enables to check several noise levels for the same time-series. This is because there is a randomness character inside the `generate_time_series` method of the `Derivator` class.\n:::\n\nThe plotting python code and the resulting time series are shown in the following tabs.\n\n::: {.panel-tabset}\n\n### Plotting code.\n\n```python\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# prepare the plot \nfig = make_subplots(\n        rows=5, \n        cols=1,\n        subplot_titles=([f\"{i}-order derivatives\" for i in range(5)]), \n        )\n\n# iterate over the derivatives and plot the time-series\nfor i in range(5):\n    if i == 0:\n            fig.add_trace(go.Scatter(x=t, y=yn, mode='lines', \n                                    line=dict(color='Black'), \n                                    opacity=0.8), row=i+1, col=1)\n    fig.add_trace(go.Scatter(x=t, y=Y[i]), row=i+1, col=1)\n\n# unify the x-axis zoom and set the sizes\nfig.update_layout(\n    showlegend=False,\n    width=800,\n    height=1200,\n    xaxis2=dict(matches='x'),\n    xaxis3=dict(matches='x'),\n    xaxis4=dict(matches='x'),\n    xaxis5=dict(matches='x')\n)\n```\n\n### The plots \n\nNotice that the first subplot shows both the noise-free and the noisy version of the time-series to be differentiated. (*The plots are zoomable*). \n\n:::{.embed}\n<iframe src=\"images/generated_plots.html\" width=\"1400\" height=\"800\"></iframe>\n:::\n\n:::\n\n## Computing the derivatives via the `derivate` method {#derivate}\n\nThe main method that is exported by the `ml_derivatives` module is the `derivate` that is described in the present section. \n\n### Input arguments \nThe table below describes the input arguments of the `derivate` method.\n\n:::{.tbl-caption}\n| **Parameter**     | **Description**      | **Default**|\n|---|---------------|----:|\n| `y` |  The noisy time-series to be differentitated to the order `d`. This should be a `numpy` array.| --|\n| `d`| Integer inside [0,1,2,3,4] representing the derivation order to be computed. Notice that the method for computing the derivation order `d` is not based on the computation of the lower order derivatives. | -- |\n| `dt`| The sampling acquisition period that should be a `float` scalar number. | -- |\n| `fit_before`| A boolean option instructing the algorithm to first evaluate the bandwidth of the time-series `y` or not before computing the derivatives. **Upon the first call, the fit is necessarily done**. For the next evaluation, the option is made available and set to `False` by default for computational efficiency reasons. The rationale is to avoid the bandwidth evaluation if the user thinks that the outcome of the first evaluation is still relevant. Swith it to `True`only if changes in the bandwidth is expected compared to the previous fit. | `False` |\n| `assess`| A boolean option. When `True` a computation of statistics of error on the $0$-th derivatives is done and based on which if the `force_level` option is set to true, the closest level of noise among those for which models are available is computed and used in the derivative estimation. Otherwise the noise level `0.05`is used by default. | `False` |\n| `force_level`| A boolean instructing the algorithm to look for the appropriate noise-level's model to be used in the estimation of the derivative when set to `True`. Otherwise the noise level `0.05`is used by default. This option is relevant only if the previous `assess` option is set to `True`. | `False` |\n| `mute_warnings`| A boolean option which, when set to `True`, Suppresses the printing of some warning messages that are not necessarily useful in production although they can be informative in the preparation phase. | `True` |\n\nTable: Input arguments for the `derivate` method of the class `ml_derivatives` module.\n:::\n\n### Output arguments \n\nThe `derivate` method of the `ml_derivatives` module returns a tuple, say `output` with the following meanings\n\n:::{.tbl-caption}\n| **parameters** | **Description** |\n|---|----------------|\n| `output[0]`| The estimated derivatives of order `d` where `d` is the derivation order used in the call of the method.|\n| `output[1]` | The sequence of confidence interval associated to the estimation. |\n| `output[2]` | A dictionary representing the statistics of errors on the $0$-derivative estimation. This is produced only if the `assess` input argument is set to `True`otherwise `output[2]=None` is returned. |\n\n\nTable: Output arguments for the `derivate` method of the class `ml_derivatives` module.\n:::\n\n## Examples of use {#example}\n\n### Use the derivatives estimator \n\nThe following script uses the previously generated time-series, add several levels of noise $\\in \\{0.02, 0.04, 0.05, 0.07\\}$ and for each resulting noisy time-series, computes the fives derivatives ($d=0,1,2,3,4$) and create the associated `plotly` figures. \n\n```python \nfor noise in [0.02, 0.04, 0.05, 0.07]:\n    yn = noise * np.random.randn(len(Y[0])) + Y[0]\n    fig = make_subplots(rows=5, cols=1, \n                        subplot_titles=(f'true and noisy signal / noise={noise}', \n                                        f'True vs estimated {1}-derivative', \n                                        f'True vs estimated {2}-derivative',\n                                        f'True vs estimated {3}-derivative',  \n                                        f'True vs estimated {4}-derivative'),\n                        shared_xaxes=True)\n\n    fig.add_trace(go.Scatter(x=t, y=Y[0], name=f'True y'), row=1, col=1)\n    fig.add_trace(go.Scatter(x=t, y=yn, name=f'noisy yn'), row=1, col=1)\n\n    for d in range(5):\n        yhat, e_std, log = der.derivate(y=yn, d=d, dt=dt)\n        #--\n        fig.add_trace(go.Scatter(x=t, y=Y[d], name=f'True {d}-derivative', \n                                    line=dict(color='Black')), row=d+1, col=1)\n        \n        fig.add_trace(go.Scatter(x=t, y=yhat, name=f'estimated {d}-derivative', \n                                    line=dict(color='Blue')), row=d+1, col=1)\n\nfig.update_layout(\n    width = 800, \n    height=1200,\n    showlegend=False\n)\nfig.write_html(f'example_{int(100 * noise)}.html')\n```\n\nThe resulting plots are shown in the tabls below: \n\n::: {.panel-tabset}\n\n### level noise = 0.02\n\n:::{.embed}\n<iframe src=\"images/example_2.html\" width=\"1400\" height=\"800\"></iframe>\n:::\n\n### level noise = 0.04\n\n:::{.embed}\n<iframe src=\"images/example_4.html\" width=\"1400\" height=\"800\"></iframe>\n:::\n\n### level noise = 0.05\n\n:::{.embed}\n<iframe src=\"images/example_5.html\" width=\"1400\" height=\"800\"></iframe>\n:::\n\n### level noise = 0.07\n\n:::{.embed}\n<iframe src=\"images/example_7.html\" width=\"1400\" height=\"800\"></iframe>\n:::\n\n:::\n\n### Use the confidence intervals provided \n\nThe confidence interval is to be used as if was an indication avout the standard deviation that is defined *point-wise* along the time interval over which the estimation of the derivative is conducted. Based on this, it is common to use as confidence intervals around the estimation that show a width of **mutiples** of the standard deviation. This takes the following form:\n\n$$\n[y_\\text{prediced}-\\rho\\sigma, y_\\text{prediced}+\\rho\\sigma]\\quad \\rho=1,2,3\n$${#eq-sigma}\n\nThe following script shows how the estimation of $\\sigma$ can be used and computes for a noisy signal the **probability** of the true value being inside the above defined interval for different values of $\\rho\\in \\{1,2,3\\}$. The thrid derivatice is used for the sake of illustration together with a noise level of 0.05. \n\n```python \nnoise = 0.05\nd = 3\nyn = noise * np.random.randn(len(Y[0])) + Y[0]\nyhat, sigma, _ = der.derivate(y=yn, d=d, dt=dt, assess=True, force_level=True)\ndic_confidence = {}\nfor rho in [1,2,3]:\n     cond1 = Y[d] <= yhat + rho * sigma\n     cond2 = Y[d] >= yhat - rho * sigma\n     dic_confidence[rho] = int(np.sum(cond1 & cond2)/len(yhat) * 100)\n\nimport pandas as pd \ndf_confidence = pd.DataFrame(dic_confidence, index=['Probability'])\ndf_confidence.columns = [f'rho={i+1}' for i in range(3)]\nprint(\"--------------------------------------------------------------\")\nprint(\"-- Probability of being inside [-rho * sigma, +rho * sigma] --\")\nprint(\"--------------------------------------------------------------\")\nprint(df_confidence)\nprint(\"--------------------------------------------------------------\")\n```\n\nwhich produces the following result: \n\n![Relevance of the confidence intervals](images/outputterminal.png){fig-align=\"left\" width=\"80%\"}\n\nwhich means for instance that **more than 98%** of the instants, the true values of the third derivatives lies inside the $2\\sigma$-defined confidence interval. \n\n\n## Limitations {#limitations}\n\n::: {.callout-warning title=\"Minimum number of `N`=50 points required.\"} \n\nAs the derivatives estimator involves an estimation of the bandwidth of the signal and since the precision of this estimation is inversly proportional to the number of points, the current implementation of the module required a minimum number od points `N`=50.\n\n:::\n\n\n::: {.callout-warning title=\"Maximum bandwidth for a given `dt`.\"} \n\nObviously, given the acquisition period `dt`, there is a limitation on the bandwidth of the signal for which derivatives can be computed. For this reason, it is not advisable to attempt estimation beyond the following maximum pulsation: \n$$\n\\omega_\\text{max}= \\dfrac{2\\pi}{5\\times \\texttt{dt}}\n$$\n\n:::\n\n::: {.callout-warning title=\"`ml_derivatives` is slower than standard filters although incomparatively more precise\"} \n\nIt is important to underline that the counterpart of the above nice features is that the processing time is not that of a point-wise filter although computation time remains descently small. See below for more details regarding standard compuation times using the `ml_derivatives` module. \n:::\n\n::: {.callout-warning title='The length of the time-series is limited to 10000'}\n\nSince the estimation is based on some stored information, for the sake of memory, the length of the sequence `yn` used as input argument in (@eq-problem) is limited to 10000. If estimation of derivatives for longer sequence is required, please decompose the signal into multiple segments. \n:::\n\n## Citing ml_derivatives {#citing}\n\n\n```bibtex\n@misc{alamir2025reconstructinghighderivativesnoisy,\n      title={On reconstructing high derivatives of noisy time-series with confidence intervals}, \n      author={Mazen Alamir},\n      year={2025},\n      eprint={2503.05222},\n      archivePrefix={arXiv},\n      primaryClass={eess.SY},\n      url={https://arxiv.org/abs/2503.05222}, \n}\n```\n\n::: {.callout-note}\nThe above reference contains a detailed description of the algorithm together with an extensive comparison with the best available alternatives. It also explain the general targeted scope of the module that mainly focus on extracting high derivatives from noisy time-series in the aim of building learning datasets that contains *virtual sensor*-like columns representing derivatives of different orders of the raw columned coming from the ground measurements. \n:::\n\n[^1]: Should be available in May 2025."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.32","theme":"flatly","title":"ML_derivatives","subtitle":"A module to compute high derivatives of noisy time-series.","author":[{"name":"Mazen Alamir","affiliation":"CNRS, University of Grenoble Alpes","homepage":"https://www.mazenalamir.fr"}],"date":"April 25, 2025","keywords":["Signal preocessing","derivatives estimation","denoising","Machine Learning","Python"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}